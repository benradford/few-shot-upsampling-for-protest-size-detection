{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, TFRobertaModel\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "seq_len = 450\n",
    "max_len = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/squad/train-v2.0.json\"\n",
    "eval_path = \"../data/squad/dev-v2.0.json\"\n",
    "\n",
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "save_path = \"roberta_base/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11873 evaluation points created.\n",
      "130319 examples. (130236, 512) training points created.\n"
     ]
    }
   ],
   "source": [
    "class SquadExample:\n",
    "    def __init__(self, question, context, start_char_idx, answer_text, all_answers, seq_len, max_len):\n",
    "        \n",
    "        # Clean context, answer and question\n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        answer = \" \".join(str(answer_text).split())\n",
    "\n",
    "        self.question = str(question)\n",
    "        self.context = str(context)\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = str(answer)\n",
    "        self.all_answers = all_answers\n",
    "        self.max_len = max_len\n",
    "        self.seq_len = seq_len\n",
    "        self.skip_doc = False\n",
    "        \n",
    "        self.input_ids = None\n",
    "        self.attention_mask = None\n",
    "        self.token_type_ids = None\n",
    "        self.start_token_idx = None\n",
    "        self.end_token_idx = None\n",
    "        self.skip = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        print(np.stack(self.input_ids).shape)\n",
    "        print(np.stack(self.token_type_ids).shape)\n",
    "        print(np.stack(self.attention_mask).shape)\n",
    "        return \"<SquadExample>\"\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return repr({\"input_ids\":self.input_ids, \n",
    "                     \"token_type_ids\":self.token_type_ids, \n",
    "                     \"attention_mask\":self.attention_mask,\n",
    "                     \"start_token_idx\":self.start_token_idx,\n",
    "                     \"end_token_idx\":self.end_token_idx,\n",
    "                     \"skip\":self.skip})\n",
    "\n",
    "    def preprocess(self):\n",
    "        context = self.context\n",
    "        question = self.question\n",
    "        answer = self.answer_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "\n",
    "        # Find end character index of answer in context\n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "        if (end_char_idx >= len(context)) or (start_char_idx < 0):\n",
    "            self.skip_doc = True\n",
    "            return\n",
    "\n",
    "        # Mark the character indexes in context that are in answer\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "            \n",
    "        # Tokenize context\n",
    "        tokenized_context = tokenizer(context, return_offsets_mapping=True)\n",
    "        \n",
    "        context_input_ids = tokenized_context.input_ids\n",
    "        context_offset_mapping = tokenized_context.offset_mapping\n",
    "        context_attention_mask = tokenized_context.attention_mask\n",
    "        \n",
    "        self.context_input_ids = context_input_ids\n",
    "        self.context_offset_mapping = context_offset_mapping\n",
    "        \n",
    "        # Find tokens that were created from answer characters\n",
    "        ans_token_idx = []\n",
    "        for idx, (start, end) in enumerate(context_offset_mapping):\n",
    "            if sum(is_char_in_ans[start:end]) > 0:\n",
    "                ans_token_idx.append(idx)\n",
    "\n",
    "#         if len(ans_token_idx) == 0:\n",
    "#             self.skip_doc = True\n",
    "#             return\n",
    "        if (len(ans_token_idx) == 0):\n",
    "            ans_token_idx = [-1]\n",
    "\n",
    "        # Find start and end token index for tokens from answer\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "        \n",
    "        self.start_token_idx_master = start_token_idx\n",
    "        self.end_token_idx_master = end_token_idx\n",
    "        \n",
    "        # Tokenize question\n",
    "        tokenized_question = tokenizer(\"</s> \"+question, return_offsets_mapping=True)\n",
    "        \n",
    "        ## Crop start and end tokens\n",
    "        question_input_ids = tokenized_question.input_ids[1:]\n",
    "        context_input_ids = context_input_ids[1:-1]\n",
    "\n",
    "        ##\n",
    "        ## SPLIT UP CONTEXT INTO MULTIPLE QUESTIONS OF max_len\n",
    "        ##\n",
    "        \n",
    "        if seq_len >= len(context_input_ids):\n",
    "            offsets = [0]\n",
    "        else:\n",
    "            ii = 0\n",
    "            offsets = []\n",
    "            while (ii+seq_len) <= len(context_input_ids):\n",
    "                offsets.append(ii)\n",
    "                ii = ii + round(seq_len/2)\n",
    "            offsets = offsets + [len(context_input_ids)-seq_len]\n",
    "        \n",
    "        list_input_ids = []\n",
    "        list_start_token_idx = []\n",
    "        list_end_token_idx = []\n",
    "        list_attention_mask = []\n",
    "        list_token_type_ids = []\n",
    "        list_skip = []\n",
    "        \n",
    "        for ii in offsets:\n",
    "            subcontext_input_ids = [0]+context_input_ids[ii:(ii+seq_len)]+question_input_ids\n",
    "            subcontext_start_token_idx = start_token_idx - ii\n",
    "            subcontext_end_token_idx = end_token_idx - ii\n",
    "            subcontext_padding = [0] * (self.max_len - len(subcontext_input_ids))\n",
    "            \n",
    "            subcontext_attention_mask = [1] * len(subcontext_input_ids) + subcontext_padding\n",
    "            subcontext_token_type_ids = [0] + [0]*len(context_input_ids[ii:(ii+seq_len)]) + [1]*len(question_input_ids) + (np.array(subcontext_padding)+1).tolist()\n",
    "            subcontext_input_ids = subcontext_input_ids + (np.array(subcontext_padding)+1).tolist()\n",
    "            \n",
    "            if (subcontext_start_token_idx >= 0) and (subcontext_end_token_idx < seq_len):\n",
    "                skip = False\n",
    "            else:\n",
    "                subcontext_start_token_idx = 0\n",
    "                subcontext_end_token_idx = 0\n",
    "                skip = False\n",
    "                \n",
    "            list_input_ids.append(subcontext_input_ids)\n",
    "            list_attention_mask.append(subcontext_attention_mask)\n",
    "            list_token_type_ids.append(subcontext_token_type_ids)\n",
    "            list_start_token_idx.append(subcontext_start_token_idx)\n",
    "            list_end_token_idx.append(subcontext_end_token_idx)\n",
    "            list_skip.append(skip)\n",
    "        \n",
    "        self.input_ids = (list_input_ids)\n",
    "        self.attention_mask = (list_attention_mask)\n",
    "        self.token_type_ids = (list_token_type_ids)\n",
    "        self.start_token_idx = (list_start_token_idx)\n",
    "        self.end_token_idx = (list_end_token_idx)\n",
    "        self.skip = list_skip\n",
    "        self.example_offset = offsets\n",
    "        \n",
    "    def train_examples(self, include_impossible=False):\n",
    "        for idx, skip_ex in enumerate(self.skip):\n",
    "            if include_impossible is False:\n",
    "                if skip_ex is False:\n",
    "                    yield  {\"input_ids\":self.input_ids[idx],\n",
    "                            \"token_type_ids\":self.token_type_ids[idx],\n",
    "                            \"attention_mask\":self.attention_mask[idx],\n",
    "                            \"start_token_idx\":self.start_token_idx[idx],\n",
    "                            \"end_token_idx\":self.end_token_idx[idx]}\n",
    "            else:\n",
    "                yield  {\"input_ids\":self.input_ids[idx],\n",
    "                            \"token_type_ids\":self.token_type_ids[idx],\n",
    "                            \"attention_mask\":self.attention_mask[idx],\n",
    "                            \"start_token_idx\":self.start_token_idx[idx],\n",
    "                            \"end_token_idx\":self.end_token_idx[idx]}\n",
    "    \n",
    "    \n",
    "    def inference_from_onehot(self, pred_start, pred_end):\n",
    "\n",
    "#         if force_answer == False:\n",
    "        if (np.max(np.argmax(pred_start, axis=1)) == 0) and (np.max(np.argmax(pred_end, axis=1)) == 0):\n",
    "            return(\"\", -1,-1,-1,-1)\n",
    "        \n",
    "        seq_len = min(self.seq_len, len(self.context_input_ids))\n",
    "        \n",
    "        pred_start_matrix = np.zeros((len(self.input_ids), len(self.context_input_ids)))\n",
    "        pred_end_matrix = np.zeros((len(self.input_ids), len(self.context_input_ids)))\n",
    "        \n",
    "        for idx, value in enumerate(pred_start):\n",
    "            offset = self.example_offset[idx]+1\n",
    "            pred_start_sub = pred_start[idx][1:seq_len]\n",
    "            pred_end_sub   = pred_end[idx][1:seq_len]\n",
    "            pred_start_matrix[idx,(offset):(offset+seq_len-1)] = pred_start_sub\n",
    "            pred_end_matrix[idx,(offset):(offset+seq_len-1)] = pred_end_sub\n",
    "            \n",
    "        highest_prob = np.argmax(np.max(pred_start_matrix, axis=1) + np.max(pred_end_matrix, axis=1))\n",
    "        \n",
    "        top_start = np.argmax(pred_start_matrix[highest_prob,:])\n",
    "        top_end   = np.argmax(pred_end_matrix[highest_prob,:])\n",
    "        \n",
    "#         pred_start = np.max(pred_start_matrix, axis=0)\n",
    "#         pred_end   = np.max(pred_end_matrix, axis=0)\n",
    "        \n",
    "#         top_start = np.argmax(pred_start)\n",
    "#         top_end = np.argmax(pred_end)\n",
    "        \n",
    "        start_char = self.context_offset_mapping[top_start][0]\n",
    "        end_char = self.context_offset_mapping[top_end][1]\n",
    "        \n",
    "        return (self.context[start_char:end_char], top_start, top_end, start_char, end_char)\n",
    "    \n",
    "                \n",
    "    def model_inference(self, model):\n",
    "        pred = model.predict([np.stack(self.input_ids),\n",
    "                      np.stack(self.attention_mask),\n",
    "                      np.stack(self.token_type_ids)], batch_size=8)\n",
    "        pred_start = pred[0]\n",
    "        pred_end   = pred[1]\n",
    "        \n",
    "        return self.inference_from_onehot(pred_start, pred_end)\n",
    "    \n",
    "    def fake_inference(self):\n",
    "        pred_start_mat = []\n",
    "        pred_end_mat = []\n",
    "        for idx, val in enumerate(self.start_token_idx):\n",
    "            pred_start = np.zeros_like(np.array(self.input_ids[idx]))\n",
    "            pred_end = np.zeros_like(np.array(self.input_ids[idx]))\n",
    "            if self.skip[idx] == False:\n",
    "                pred_start[val] = 1.0\n",
    "                pred_end[self.end_token_idx[idx]] = 1.0\n",
    "            pred_start_mat.append(pred_start)\n",
    "            pred_end_mat.append(pred_end)\n",
    "            \n",
    "        pred_start_mat = np.array(pred_start_mat)\n",
    "        pred_end_mat = np.array(pred_end_mat)\n",
    "        \n",
    "        return self.inference_from_onehot(pred_start_mat,pred_end_mat)\n",
    "        \n",
    "\n",
    "def create_squad_examples(raw_data, seq_len, max_len):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if len(qa[\"answers\"]) > 0:\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    squad_eg = SquadExample(\n",
    "                        question, context, start_char_idx, answer_text, all_answers, seq_len, max_len\n",
    "                    )\n",
    "                    squad_eg.preprocess()\n",
    "                    squad_examples.append(squad_eg)\n",
    "                else:\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = \"\"\n",
    "                    all_answers = [\"\"]\n",
    "                    start_char_idx = 0\n",
    "                    squad_eg = SquadExample(\n",
    "                        question, context, start_char_idx, answer_text, all_answers, seq_len, max_len\n",
    "                    )\n",
    "                    squad_eg.preprocess()\n",
    "                    squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples, include_impossible=False):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip_doc is False:\n",
    "            for example in item.train_examples(include_impossible):\n",
    "                for key in dataset_dict:\n",
    "                    dataset_dict[key].append(np.array(example[key]))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = (\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"])\n",
    "    y = (dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"])\n",
    "    return x, y\n",
    "\n",
    "def merge_squad_results(squad_examples, start_preds, end_preds):\n",
    "    ii = 0\n",
    "    tally = []\n",
    "    for ex in squad_examples:\n",
    "        if ex.skip_doc is False:\n",
    "            n_sub = len(ex.skip)\n",
    "            pred_out = ex.inference_from_onehot(start_preds[ii:(ii+n_sub),:], end_preds[ii:(ii+n_sub),:])[0]\n",
    "            tally.append(pred_out in ex.all_answers)\n",
    "            ii = ii + n_sub\n",
    "    return np.mean(tally)\n",
    "\n",
    "\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "\n",
    "print(\"Here\")\n",
    "\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data, seq_len, max_len)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples, include_impossible=True)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "\n",
    "\n",
    "train_squad_examples = create_squad_examples(raw_train_data, seq_len, max_len)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples, include_impossible=False)\n",
    "print(f\"{len(train_squad_examples)} examples. {x_train[0].shape} training points created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ## BERT encoder\n",
    "    encoder = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    embedding = encoder.roberta(\n",
    "        input_ids, \n",
    "        token_type_ids=token_type_ids, \n",
    "        attention_mask=attention_mask\n",
    "    )[0]\n",
    "\n",
    "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, \n",
    "                token_type_ids, \n",
    "                attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 124645632   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "start_logit (Dense)             (None, 512, 1)       768         roberta[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "end_logit (Dense)               (None, 512, 1)       768         roberta[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           start_logit[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           end_logit[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 512)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 512)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 124,647,168\n",
      "Trainable params: 124,647,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "use_tpu = False\n",
    "if use_tpu:\n",
    "    # Create distribution strategy\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "else:\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 196 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 196 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "9768/9768 [==============================] - 4039s 413ms/step - activation_4_loss: 1.2149 - loss: 2.3654 - activation_5_loss: 1.1505 - val_activation_4_loss: 1.0815 - val_loss: 2.0843 - val_activation_5_loss: 1.0028\n",
      "Epoch 2/3\n",
      "9768/9768 [==============================] - 4049s 415ms/step - activation_4_loss: 0.8561 - loss: 1.6548 - activation_5_loss: 0.7987 - val_activation_4_loss: 0.9756 - val_loss: 1.8872 - val_activation_5_loss: 0.9116\n",
      "Epoch 3/3\n",
      "9768/9768 [==============================] - 4044s 414ms/step - activation_4_loss: 0.7435 - loss: 1.4310 - activation_5_loss: 0.6875 - val_activation_4_loss: 1.0559 - val_loss: 2.0373 - val_activation_5_loss: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6a18227780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact_match_callback = ExactMatch(x_eval, y_eval)\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=3,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=1,\n",
    "    batch_size=12,\n",
    "    validation_split=0.1,\n",
    "#     callbacks=[exact_match_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ben/anaconda3/envs/distant_crowds/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: roberta_base_squad2_512in/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"roberta_base_squad2_512in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 [==============================] - 100s 301ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.736495611073599"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds = model.predict(x_eval, batch_size=36, verbose=True)\n",
    "merge_squad_results(eval_squad_examples, eval_preds[0], eval_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618/3618 [==============================] - 1175s 325ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.782126177227677"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = model.predict(x_train, batch_size=36, verbose=1)\n",
    "merge_squad_results(train_squad_examples, train_preds[0], train_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:distant_crowds]",
   "language": "python",
   "name": "conda-env-distant_crowds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
